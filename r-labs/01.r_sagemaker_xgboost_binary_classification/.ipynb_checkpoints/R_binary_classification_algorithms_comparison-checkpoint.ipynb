{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fd44cc",
   "metadata": {},
   "source": [
    "## Compare built-in Sagemaker classification algorithms for a binary classification problem using Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f342df",
   "metadata": {},
   "source": [
    "> This notebook is based on Amazon SageMaker example notebook - [R_binary_classification_algorithms_comparison.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/r_examples/r_sagemaker_binary_classification_algorithms/R_binary_classification_algorithms_comparison.ipynb)\n",
    "\n",
    "In the notebook tutorial, we build a XGBoost classification model using HPO and evaluate deployment model using test dataset.\n",
    "\n",
    "IRIS is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. The dataset is built-in by default into R or can also be downloaded from https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "The iris dataset, besides its historical importance, is also a fun dataset to play with since it can educate us about various ML techniques such as clustering, classification and regression, all in one dataset.\n",
    "\n",
    "The dataset is built into any base R installation, so no download is required.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. Species of flowers: Iris setosa, Iris versicolor, Iris virginica\n",
    "\n",
    "The prediction we will perform is `Species ~ f(sepal.length,sepal.width,petal.width,petal.length)`\n",
    "\n",
    "Predicted attribute: Species of iris plant.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bc401",
   "metadata": {},
   "source": [
    "### Load required libraries and initialize variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96595e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "library(reticulate) # be careful not to install reticulate again. since it can cause problems.\n",
    "library(tidyverse)\n",
    "library(pROC)\n",
    "set.seed(1324)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef8e97",
   "metadata": {},
   "source": [
    "SageMaker needs to be imported using the **[reticulate](https://rstudio.github.io/reticulate/)** library. If this was performed in a local computer, we would have to make sure that Python and appropriate SageMaker libraries are installed, but inside a SageMaker notebook R kernels, these are all pre-loaded and the R user does not have to worry about installing reticulate or Python. \n",
    "\n",
    "Session is the unique session ID associated with each SageMaker call. It remains the same throughout the execution of the program and can be recalled later to close a session or open a new session.\n",
    "\n",
    "The bucket is the Amazon S3 bucket where we will be storing our data output. The Amazon S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "The role is the role of the SageMaker notebook as when it was initially deployed. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with appropriate full IAM role arn string(s).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a91cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker <- import('sagemaker')\n",
    "session <- sagemaker$Session()\n",
    "bucket <- session$default_bucket() # you may replace with name of your personal S3 bucket\n",
    "role_arn <- sagemaker$get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac95c7",
   "metadata": {},
   "source": [
    "### Input the data and basic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be74fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190824ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0391e7e",
   "metadata": {},
   "source": [
    "In above, we see that there are 50 flowers of the setosa species, 50 flowers of the versicolor species, and 50 flowers of the virginica species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a67ad2",
   "metadata": {},
   "source": [
    "In this case, the target variable is the Species prediction. We are trying to predict the species of the flower given its numerical measurements of Sepal length, sepal width, petal length, and petal width. Since we are trying to do binary classification, we will only take the flower species setosa and versicolor for simplicity. Also we will perform one-hot encoding on the categorical variable Species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699c5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris1 <- iris %>% \n",
    "    dplyr::select(Species,Sepal.Length,Sepal.Width,Petal.Length,Petal.Width) %>% # change order of columns such that the label column is the first column.\n",
    "    dplyr::filter(Species %in% c(\"setosa\",\"versicolor\")) %>%                     #only select two flower for binary classification.\n",
    "    dplyr::mutate(Species = as.numeric(Species) -1)                              # one-hot encoding,starting with 0 as setosa and 1 as versicolor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338575f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head(iris1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82954ca2",
   "metadata": {},
   "source": [
    "We now obtain some basic descriptive statistics of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47022b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris1 %>% group_by(Species) %>% summarize(mean_sepal_length = mean(Sepal.Length),\n",
    "                                         mean_petal_length = mean(Petal.Length),\n",
    "                                         mean_sepal_width = mean(Sepal.Width),\n",
    "                                         mean_petal_width = mean(Petal.Width),\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e492f0",
   "metadata": {},
   "source": [
    "In the summary statistics, we observe that mean sepal length is longer than mean petal length for both flowers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7833b26",
   "metadata": {},
   "source": [
    "### Prepare for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b7a46",
   "metadata": {},
   "source": [
    "##### We split the train and test and validate into 70%, 15%, and 15%, using random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6d609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_train <- iris1 %>%\n",
    "                    sample_frac(size = 0.7)\n",
    "iris_test <- anti_join(iris1, iris_train) %>%  \n",
    "                  sample_frac(size = 0.5)\n",
    "iris_validate <- anti_join(iris1, iris_train) %>%\n",
    "                        anti_join(., iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55124158",
   "metadata": {},
   "source": [
    "##### We do a check of the summary statistics to make sure train, test, validate datasets are appropriately split and have proper class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a2cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table(iris_train$Species)\n",
    "nrow(iris_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b970cf0",
   "metadata": {},
   "source": [
    "We see that the class balance between 0 and 1 is almost 50% each for the binary classification. We also see that there are 70 rows in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d6b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table(iris_validate$Species)\n",
    "nrow(iris_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba950db",
   "metadata": {},
   "source": [
    "We see that the class balance in validation dataset between 0 and 1 is almost 50% each for the binary classification. We also see that there are 15 rows in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76514ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table(iris_test$Species)\n",
    "nrow(iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2271689",
   "metadata": {},
   "source": [
    "We see that the class balance in test dataset between 0 and 1 is almost 50% each for the binary classification. We also see that there are 15 rows in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307a959",
   "metadata": {},
   "source": [
    "### Write the data to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55026467",
   "metadata": {},
   "source": [
    "Different algorithms in SageMaker will have different data formats required for training and for testing. These formats are created to make model production easier. csv is the most well known of these formats and has been used here as input in all algorithms to make it consistent.\n",
    "\n",
    "SageMaker algorithms take in data from an Amazon S3 object and output data to an Amazon S3 object, so data has to be stored in Amazon S3 as csv,json, proto-buf or any format that is supported by the algorithm that you are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25eabdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_csv(iris_train, 'iris_train.csv', col_names = FALSE)\n",
    "write_csv(iris_validate, 'iris_valid.csv', col_names = FALSE)\n",
    "write_csv(iris_test, 'iris_test.csv', col_names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63db1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_train <- session$upload_data(path = 'iris_train.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = 'data')\n",
    "s3_valid <- session$upload_data(path = 'iris_valid.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = 'data')\n",
    "\n",
    "s3_test <- session$upload_data(path = 'iris_test.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df2b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train,\n",
    "                                     content_type = 'text/csv')\n",
    "s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_valid,\n",
    "                                     content_type = 'text/csv')\n",
    "s3_test_input <- sagemaker$inputs$TrainingInput(s3_data = s3_test,\n",
    "                                     content_type = 'text/csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2542c",
   "metadata": {},
   "source": [
    "To perform Binary classification on Tabular\tdata using SageMaker built-in algorithm XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f88a2",
   "metadata": {},
   "source": [
    "## Create XGBoost model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ff2bc",
   "metadata": {},
   "source": [
    "Use the XGBoost built-in algorithm to build an XGBoost training container as shown in the following code example. You can automatically spot the XGBoost built-in algorithm image URI using the SageMaker image_uris.retrieve API (or the get_image_uri API if using Amazon SageMaker Python SDK version 1). If you want to ensure if the image_uris.retrieve API finds the correct URI, see Common parameters for built-in algorithms and look up XGBoost from the full list of built-in algorithm image URIs and available regions.\n",
    "\n",
    "After specifying the XGBoost image URI, you can use the XGBoost container to construct an estimator using the SageMaker Estimator API and initiate a training job. This XGBoost built-in algorithm mode does not incorporate your own XGBoost training script and runs directly on the input datasets.\n",
    "\n",
    "See https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29145cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container <- sagemaker$image_uris$retrieve(framework='xgboost', region= session$boto_region_name, version='latest')\n",
    "cat('XGBoost Container Image URL: ', container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07325371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_output <- paste0('s3://', bucket, '/output_xgboost')\n",
    "estimator1 <- sagemaker$estimator$Estimator(image_uri = container,\n",
    "                                           role = role_arn,\n",
    "                                           train_instance_count = 1L,\n",
    "                                           train_instance_type = 'ml.m5.2xlarge',\n",
    "                                           input_mode = 'File',\n",
    "                                           output_path = s3_output,\n",
    "                                           output_kms_key = NULL,\n",
    "                                           base_job_name = NULL,\n",
    "                                           sagemaker_session = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b6207",
   "metadata": {},
   "source": [
    "How would an untuned model perform compared to a tuned model? Is it worth the effort? Before going deeper into XGBoost model tuning, let’s highlight the reasons why you have to tune your model. The main reason to perform hyper-parameter tuning is to increase predictability of our models by choosing our hyperparameters in a well thought manner. There are 3 ways to perform hyperparameter tuning: grid search, random search, bayesian search. Popular packages like scikit-learn use grid search and random search techniques. SageMaker uses Bayesian search techniques.\n",
    "\n",
    "We need to choose \n",
    "\n",
    "- a learning objective function to optimize during model training\n",
    "- an eval_metric to use to evaluate model performance during validation\n",
    "- a set of hyperparameters and a range of values for each to use when tuning the model automatically\n",
    "\n",
    "SageMaker XGBoost model can be tuned with many hyperparameters. The hyperparameters that have the greatest effect on optimizing the XGBoost evaluation metrics are: \n",
    "\n",
    "- alpha, \n",
    "- min_child_weight, \n",
    "- subsample, \n",
    "- eta, \n",
    "- num_round.\n",
    "\n",
    "\n",
    "The hyperparameters that are required are num_class (the number of classes if it is a multi-class classification problem) and num_round ( the number of rounds to run the training on). All other hyperparameters are optional and will be set to default values if it is not specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33f178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check to make sure which are required and which are optional\n",
    "estimator1$set_hyperparameters(eval_metric='auc',\n",
    "                              objective='binary:logistic',\n",
    "                              num_round = 6L\n",
    "                              )\n",
    "\n",
    "# Set Hyperparameter Ranges, check to make sure which are integer and which are continuos parameters. \n",
    "hyperparameter_ranges = list('eta' = sagemaker$parameter$ContinuousParameter(0,1),\n",
    "                        'min_child_weight'= sagemaker$parameter$ContinuousParameter(0,10),\n",
    "                        'alpha'= sagemaker$parameter$ContinuousParameter(0,2),\n",
    "                        'max_depth'= sagemaker$parameter$IntegerParameter(0L,10L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42771dbc",
   "metadata": {},
   "source": [
    "The evaluation metric that we will use for our binary classification purpose is validation:auc, but you could use any other metric that is right for your problem. You do have to be careful to change your objective_type to point to the right direction of Maximize or Minimize according to the objective metric you have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f2add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a hyperparamter tuner\n",
    "objective_metric_name = 'validation:auc'\n",
    "tuner1 <- sagemaker$tuner$HyperparameterTuner(estimator1,\n",
    "                                             objective_metric_name,\n",
    "                                             hyperparameter_ranges,\n",
    "                                             objective_type='Maximize',\n",
    "                                             max_jobs=4L,\n",
    "                                             max_parallel_jobs=2L)\n",
    "\n",
    "# Define the data channels for train and validation datasets\n",
    "input_data <- list('train' = s3_train_input,\n",
    "                   'validation' = s3_valid_input)\n",
    "\n",
    "# train the tuner\n",
    "tuner1$fit(inputs = input_data, \n",
    "           job_name = paste('tune-xgb', format(Sys.time(), '%Y%m%d-%H-%M-%S'), sep = '-'), \n",
    "           wait=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79451cba",
   "metadata": {},
   "source": [
    "The output of the tuning job can be checked in SageMaker if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ea863",
   "metadata": {},
   "source": [
    "### Calculate AUC for the test data on XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1951de",
   "metadata": {},
   "source": [
    "SageMaker will automatically recognize the training job with the best evaluation metric and load the hyperparameters associated with that training job when we deploy the model. One of the benefits of SageMaker is that we can easily deploy models in a different instance than the instance in which the notebook is running. So we can deploy into a more powerful instance or a less powerful instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8596325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_endpoint1 <- tuner1$deploy(initial_instance_count = 1L,\n",
    "                                   instance_type = 'ml.t2.medium')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181de542",
   "metadata": {},
   "source": [
    "The serializer tells SageMaker what format the model expects data to be input in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d97e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_endpoint1$serializer <- sagemaker$serializers$CSVSerializer(content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec87668",
   "metadata": {},
   "source": [
    "We input the `iris_test` dataset without the labels into the model using the `predict` function and check its AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ce0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the test sample for input into the model\n",
    "test_sample <- as.matrix(iris_test[-1])\n",
    "dimnames(test_sample)[[2]] <- NULL\n",
    "\n",
    "# Predict using the deployed model\n",
    "predictions_ep <- model_endpoint1$predict(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410dd45-8ab4-4d32-a9ec-2b132febd97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45894fb-9a21-4b69-960f-5af5c35af015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_ep <- stringr::str_split(as.character(predictions_ep, encoding=\"utf-8\"), pattern = ',', simplify = TRUE)\n",
    "predictions_ep <- as.numeric(predictions_ep > 0.5)\n",
    "\n",
    "# Add the predictions to the test dataset.\n",
    "iris_predictions_ep1 <- dplyr::bind_cols(predicted_flower = predictions_ep, \n",
    "                      iris_test)\n",
    "iris_predictions_ep1\n",
    "\n",
    "# Get the AUC\n",
    "auc(roc(iris_predictions_ep1$predicted_flower,iris_test$Species))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9dd325",
   "metadata": {},
   "source": [
    "## Clean up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7ef60",
   "metadata": {},
   "source": [
    "##### We close the endpoints which we created to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint1$delete_model()\n",
    "\n",
    "session$delete_endpoint(model_endpoint1$endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7bcb3-c252-4f13-a82d-43ca1f2b8d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
